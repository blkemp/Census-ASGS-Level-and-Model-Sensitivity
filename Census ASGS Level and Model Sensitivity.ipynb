{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Census WFH   \n",
    "The goal of this workbook is to compare the efficacy of machine learning models based on different levels of spacial consolidation, based on the [Australian Statistical Geography Standard](https://www.abs.gov.au/websitedbs/D3310114.nsf/home/Australian+Statistical+Geography+Standard+(ASGS)) framework. the key questions are:   \n",
    "\n",
    "- Can we find a model for predicting with an R2 value > 0.7? (Based on [prior work](https://github.com/blkemp/ABS-Region-Data/tree/BKSubmission), I suspect the answer is yes).\n",
    "- Using this model, what is the impact on accuracy for feeding in data that is consolidated at a different level (e.g. neighborhood vs city vs county)?\n",
    "- How do models trained at differing levels of granularity compare in both baseline accuracy and generalisability? I.e. Are models trained with the most fine grained data more accurate, or are they prone to overfitting?\n",
    "\n",
    "As a starting point I will be utilising the [Australian Bureau of Statistics 2016 Census Datapacks](https://datapacks.censusdata.abs.gov.au/datapacks/) and attempting to predict \"working from home\" behaviours by region. Why this particular response vector? a) It just seems interesting and b) I suspect that demographic information available within the census itself (gender, age, profession and industry) will all be strongly related to both individuals' propensity to undertake working from home and their ability to do so with support from employers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "# Declare Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "import os\n",
    "from textwrap import wrap\n",
    "import operator\n",
    "\n",
    "# Set a variable for current notebook's path for various loading/saving mechanisms\n",
    "nb_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_plot_h(importances, X_train, n_features):\n",
    "    \n",
    "    # Identify the n most important features\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    columns = X_train.columns.values[indices[:n_features]]\n",
    "    values = importances[indices][:n_features]\n",
    "    \n",
    "    columns = [ '\\n'.join(wrap(c, 20)) for c in columns ]\n",
    "    \n",
    "    # Create the plot\n",
    "    fig = plt.figure(figsize = (9,n_features))\n",
    "    plt.title(\"Normalized Weights for {} Most Predictive Features\".format(n_features), fontsize = 16)\n",
    "    plt.barh(np.arange(n_features), values, height = 0.6, align=\"center\", color = '#00A000', \n",
    "          label = \"Feature Weight\")\n",
    "    plt.barh(np.arange(n_features) - 0.3, np.cumsum(values), height = 0.2, align = \"center\", color = '#00A0A0', \n",
    "          label = \"Cumulative Feature Weight\")\n",
    "    plt.yticks(np.arange(n_features), columns)\n",
    "    plt.xlabel(\"Weight\", fontsize = 12)\n",
    "    \n",
    "    plt.legend(loc = 'upper right')\n",
    "    \n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_impact_plot(model, X_train, n_features, y_label):\n",
    "    '''\n",
    "    Takes a trained model and training dataset and synthesises the impacts of the top n features\n",
    "    to show their relationship to the response vector (i.e. how a change in the feature changes\n",
    "    the prediction). Returns n plots showing the variance for min, max, median, 1Q and 3Q.\n",
    "    \n",
    "    INPUTS\n",
    "    model = Trained model in sklearn with  variable \".feature_importances_\". Trained supervised learning model.\n",
    "    X_train = Pandas Dataframe object. Feature set the training was completed using.\n",
    "    n_features = Int. Top n features you would like to plot.\n",
    "    y_label = String. Description of response variable for axis labelling.\n",
    "    '''\n",
    "    # Display the n most important features\n",
    "    indices = np.argsort(model.feature_importances_)[::-1]\n",
    "    columns = X_train.columns.values[indices[:n_features]]\n",
    "    \n",
    "    sim_var = [[]]\n",
    "    \n",
    "    for col in columns:\n",
    "        base_pred = model.predict(X_train)\n",
    "        #add percentiles of base predictions to a df for use in reporting\n",
    "        base_percentiles = [np.percentile(base_pred, pc) for pc in range(0,101,25)]\n",
    "\n",
    "        # Create new predictions based on tweaking the parameter\n",
    "        # copy X, resetting values to align to the base information through different iterations\n",
    "        df_copy = X_train.copy()\n",
    "\n",
    "        for val in np.arange(-X_train[col].std(), X_train[col].std(), X_train[col].std()/50):\n",
    "            df_copy[col] = X_train[col] + val\n",
    "            # Add new predictions based on changed database\n",
    "            predictions = model.predict(df_copy)\n",
    "            \n",
    "            # Add percentiles of these predictions to a df for use in reporting\n",
    "            percentiles = [np.percentile(predictions, pc) for pc in range(0,101,25)]\n",
    "            \n",
    "            # Add variances between percentiles of these predictions and the base prediction to a df for use in reporting\n",
    "            percentiles = list(map(operator.sub, percentiles, base_percentiles))\n",
    "            percentiles = list(map(operator.truediv, percentiles, base_percentiles))\n",
    "            sim_var.append([val, col] + percentiles)\n",
    "\n",
    "    # Create a dataframe based off the arrays created above\n",
    "    df_predictions = pd.DataFrame(sim_var,columns = ['Value','Feature']+[0,25,50,75,100])\n",
    "    \n",
    "    # Create a subplot object based on the number of features\n",
    "    num_cols = 2\n",
    "    subplot_rows = int(n_features/num_cols) + int(n_features%num_cols)\n",
    "    fig, axs = plt.subplots(nrows = subplot_rows, ncols = num_cols, sharey = True, figsize=(15,5*subplot_rows))\n",
    "\n",
    "    nlines = 1\n",
    "\n",
    "    # Plot the feature variance impacts\n",
    "    for i in range(axs.shape[0]*axs.shape[1]):\n",
    "        if i < len(columns):\n",
    "            # Cycle through each plot object in the axs array and plot the appropriate lines\n",
    "            ax_row = int(i/num_cols)\n",
    "            ax_column = int(i%num_cols)\n",
    "            \n",
    "            axs[ax_row, ax_column].plot(df_predictions[df_predictions['Feature'] == columns[i]]['Value'],\n",
    "                     df_predictions[df_predictions['Feature'] == columns[i]][50])\n",
    "            \n",
    "            axs[ax_row, ax_column].set_title(\"\\n\".join(wrap(columns[i], int(100/num_cols))))\n",
    "            \n",
    "            # Create spacing between charts if chart titles happen to be really long.\n",
    "            nlines = max(nlines, axs[ax_row, ax_column].get_title().count('\\n'))\n",
    "\n",
    "            axs[ax_row, ax_column].set_xlabel('Simulated +/- change to feature'.format(y_label))\n",
    "            \n",
    "            # Format the y-axis as %\n",
    "            if ax_column == 0:\n",
    "                vals = axs[ax_row, ax_column].get_yticks()\n",
    "                axs[ax_row, ax_column].set_yticklabels(['{:,.2%}'.format(x) for x in vals])\n",
    "                axs[ax_row, ax_column].set_ylabel('% change to {}'.format(y_label))\n",
    "        \n",
    "        # If there is a \"spare\" plot, hide the axis so it simply shows ans an empty space\n",
    "        else:\n",
    "            axs[int(i/num_cols),int(i%num_cols)].axis('off')\n",
    "    \n",
    "    # Apply spacing between subplots in case of very big headers\n",
    "    fig.subplots_adjust(hspace=0.5*nlines)\n",
    "    \n",
    "    # Return the plot\n",
    "    plt.tight_layout()    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_series_abs(S):\n",
    "    'Takes a pandas Series object and returns the series sorted by absolute value'\n",
    "    temp_df = pd.DataFrame(S)\n",
    "    temp_df['abs'] = temp_df.iloc[:,0].abs()\n",
    "    temp_df.sort_values('abs', ascending = False, inplace = True)\n",
    "    return temp_df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin importing and exploring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import metadata sheets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SA2 level data \n",
    "    # (start modelling at this level as a nice mid-point [plus it's the level I worked with in the previous project])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Come up with some heuristics on what fields to drop and what to keep\n",
    "# E.g. do you really want Age splits? If so, then you need to filter out any \"Total\" Lines for them\n",
    "# if not then you need to filter out any lines with a metadata Long cell descriptor including the word \"[A/a]ge\"\n",
    "# Similar for splits by sex, whether you choose to delete the categories of \"Persons\" or only keep this field\n",
    "\n",
    "# Come up with a function to show the levels of data available within a table, i.e. age, sex, occupation, etc.\n",
    "# Build off this to create a function to filter for these levels as desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalise this importation method to allow easy imports based on folder name (for SA level) \n",
    "# and a list of datapack files you want to amalgamate into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new \"Work From Home Participation Rate\" vector to ensure consistency across regions\n",
    "# Base this off population who worked from home divided by total working population in the region\n",
    "'Worked_home_P'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate correlations to check out items which stand out as potential drivers\n",
    "response_vector = 'INSERTVECTORHERE'\n",
    "sort_series_abs(df.dropna(subset=[solar]).corr().loc[:,response_vector])[1:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X & y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'features' and 'income' data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "parameters = {'n_estimators':[10,20,40,80],\n",
    "              #'max_depth':[4,8,16,32,64],\n",
    "              'min_samples_leaf':[1,2,4]\n",
    "             }\n",
    "\n",
    "# TODO: Make a scoring object using make_scorer()\n",
    "scorer = make_scorer(r2_score)\n",
    "\n",
    "# TODO: Perform grid search on the regressor using 'scorer' as the scoring method using GridSearchCV()\n",
    "grid_obj = GridSearchCV(rf, param_grid=parameters, scoring=scorer, verbose = 2)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "best_rf = grid_fit.best_estimator_\n",
    "\n",
    "# Make predictions using the unoptimized and model\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "best_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_pred, y_test)\n",
    "plt.xlabel('Predictions')\n",
    "plt.ylabel('Actuals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_plot_h(best_rf.feature_importances_, X_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Space for initial thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
